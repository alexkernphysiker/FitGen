\documentclass[a4paper]{article}
\usepackage [english]{babel}
\usepackage{hyperref}
\usepackage{moreverb}
%
\topmargin -60pt \oddsidemargin -0.4mm \evensidemargin -5.4mm
\setlength{\textheight}{257mm} \setlength{\textwidth}{165mm}
\setlength{\marginparsep}{8mm} \setlength{\marginparwidth}{18mm}
%
\begin{document}
This document contains the descriptions of c++ library that can be downloaded
\href{https://github.com/alexkernphysiker/FitGen}{here}.
\\
%======================================================================================
\begin{center}\Large \textbf{FitGen}\end{center}
%======================================================================================
This library contains the implementation of genetic algorithm for approximation of a set of points by a parametric function. 
The source of inspiration for developing this code can be found by the \href{http://habrahabr.ru/post/171751/}{link} (russian). 
Description of algorithm was found \href{http://www.drdobbs.com/database/differential-evolution/184410166}{here} (english).
\\
\textit{\Large File fit$\_$gen.h} contains basic classes.
\\
Class \textbf{ParamSet} contains information about a set of parameters. 
Default constructor creates an empty one.
Operator \textit{$<<$} adds a parameter into the set. The order of parameters does matter.\\
\textit{Count} returns the quantity of parameters in the set.\\
Operator \textit{[]} gets parameter value by it's number.\\
Abstract class \textbf{IParamCheck} is an interface that provides method \textit{bool CorrectParams(ParamSet$\&$)} for controlling the correctness of parameters from a \textbf{ParamSet}.
Three classes below inherit it.
\\
Abstract class \textbf{IParamFunc} is an interface that provides methods for getting the parametric function values and control the correctness of the varied parameters for it.
\\
Abstract class \textbf{IInitialConditions} is an interface for generating \textbf{ParamSet}'s for initial parameters distribution.
\\
Abstract class \textbf{IOptimalityFunction} is an interface for obtaining the optimality criterium.
Operator \textit{()} method must provide this algorithm. 
It's realizations should contain information about points which are approximated by function, though other algorithms can also be implemented.
\\
The abstract class \textbf{$\_$gen} implements the main part of genetic algorithm of approximation.
It's protected constructor requires \textit{IParamFunc} (approximating function) and \textit{IOptimalityFunction} (the object that provides algorithm of optimality criteria obtaining).
\\
\textit{SetFilter} method may set control of varied parameter values. Accepts any \textbf{IParamCheck} instance. 
It's recommended to call it before \textit{Init} method.
\\
The \textit{Init} method initializes the algorithm by set of N points in the space of varied parameters, which are created using given \textbf{IInitialConditions} object.
\\
Method \textit{Iterate} provides one algorithm's iteration.
From every element in the population, the new one is born due to algorithm implemented in the derived class.
So the population becomes two times larger.
Then the best half is selected due to \textit{IOptimalityFunction} object used.
Other points are disposed.
User has to manage the whole algorithm's work on his own. And call this method as many times as it's needed.
The parameter given to this method means the number of threads using for the calculation.
\\
There are also public methods and operators that can be used for obtaining calculated results: optimality criterium, parameter values and their dispersions and also calculate optimal function values.\\
\textit{count()} gets the count of parameters that are fitted.\\
\textit{[]} gets a parameter from optimal parameter set by number.\\
\textit{GetParameters(int)} gets one of points in fitted parameters space obtained by the algorithm. Index corresponds to the order of degradation of optimality criteria.Zero index corresponds to set of parameters obtained by \textit{[]}.\\
\textit{()} that requires measured parameters obtains the values of best-fit function.\\
\textit{GetParametersDispersion()}returns the \textbf{ParamSet} with the dispersions of fitted parameters.
\\
Every derivative class must implement the abstract virtual method \textit{born} for providing the algorithm of creating child points in the population.
\\
Class \textbf{FitGenVeg} is derivative from \textbf{$\_$gen} and contains the default implementation of \textit{born} method. Child's parameters are obtained by the formula:
\begin{equation}
C^*_i = C_i + 
M_{0,i}(A_i - B_i) + 
RandomGauss(C_i * M_{1,i},0) + 
RandomGauss(M_{2,i},0) 
\label{born_veg}
\end{equation}
where $C$ is the parent, $C^*$ is the child, $A$ and $B$ are two randomly selected members of population, $RandomGauss(\sigma, mean)$ is a random value distributed by Gauss, $M_{k,i}$ are the mutation coefficients stored as \textit{ParamSet}s.
Methods \textit{Mutation($k$)} and \textit{SetMutation($M_k$,$k$)} can be used for obtaining and setting value of mutation parameters used in the algorithm. They can be changed between iterations.
\\
Class \textbf{FitGen} is derivative from \textbf{FitGenVeg} and uses more complicated algorithm of creating new points in population. After calculating the parameters of $C^*$ from the formula \ref{born_veg} it takes Random point $X$ from the population and randomly replaces some of values in $C^*$ with values from $X$. The probability of each replacement is independent and equal to $\frac{1}{2}$.
\\
\textit{\Large File fitpoints.h}\\
contains headers of classes that inherit \textbf{IOptimalityFunction} and provide algorithms for calculating optimality criterium for function approximating the set of points that is store inside these classes.
\\
Abstract class \textbf{FitPointsAbstract} is base class for them all and has one unimplemented operator \textit{()} that is an interface operator for all \textbf{IOptimalityFunction}s. It has implemented methods for adding measured points that contain from a \textbf{ParamSet}, double function value and points weight (algorithm of $\chi^2$ calculation considers it to be an error value).
\\
There's also a possibility to store also a \textbf{ParamSet} of weights for each point that correspond to measured parameters.
One of $\chi^2$ calculation algorithm handles them as measured parameters errors.
\\
Here are also headers of two derived classes.\\
Class \textbf{SquareDiff} calculates optimality criteria by the formula
\begin{equation}\Large
S = \sum_{i=0}^{n-1} w_i (f(x_i, p)-y_i)^2
\end{equation}
where $x_i$ is the set of measured parameters from $i$-th point, $y_i$ and $w_i$ are the function value and the weight from $i$-th point. $f(x,p)$ is the fitting function that requires set of measured parameters $x$ that is obtained from the points added and the set of varied parameters $p$ that is obtained by genetic algorithm.
\\
Class \textbf{xi$\_$2} calculates optimality criteria another way (this is $\chi^2$)
\begin{equation}\Large
S = \frac{\sum_{i=0}^{n-1}(\frac{y_i-f(x_i,p)}{w_i})^2}{n - size(p)}
\label{chi_2}
\end{equation}
where $w$ is considered to contain error values as is was mentioned above. $size(p)$ is the number of parameters in $p$ (fitted parameters).
\\
\textbf{xi$\_$2$\_$wx} is also used for calculating $\chi^2$ but handles the errors of measured parameters:
\begin{equation}\Large
S = \frac{\sum_{i=0}^{n-1}\frac{(y_i-f(x_i,p))^2}{w_i^2+\sum_j{(\frac{\delta f(x_i,p)}{\delta x_{*,j}}\Delta x_{i,j})^2}}}{n - size(p)},
\label{chi_2_ex}
\end{equation}
where $i$ is the point index and $j$ is measured parameter index.
Index $*$ instead of $i$ in partial derivative is used for showing that it does not depend on point index but just on parameter value.
$\Delta x_{i,j}$ is the experimental error of measured parameter value.\\
This header file contains also several useful templates.\\
\textit{SelectFitPoints} selects points from the source that match the condition specified by \textbf{IParamCheck} instance (second parameter).
\\
\textit{SelectFitPointsByY} differs from template function mentioned above by the thing that they require function or other object providing $()$-operator that takes $double$ (Y-value) and returns $bool$ (the condition for function values).
\\
Template class \textbf{FitPoints} can be derived from any \textbf{FitPointsAbstract} instance having default constructor (without parameters) and has template constructors that fill the class with points. 
One can fill this class instance with such points that $X$ (one measured parameter)and $Y$ values are taken from objects can provide double values via \textit{[]} operator.
Constructors also require either the number of parameters or specify the range of processed indexes.
\\
Template class \textbf{FitPointsWithErrors} is the same as previous, but provides filling points also with third weight ($W$) parameter.
\\
Template class \textbf{Distribution1D} can be derived from any \textbf{FitPointsAbstract} instance having default constructor (without parameters) and provides \textit{AddValue} method for calculating the distribution density for the added values.
It's considered that the distribution density depends on one parameter $x$.
The constructor requires the range of added values (min. and max.) and the number of bins.
So for fitting one will have as many points as many bins there are.
The $x$-value of each point is the center of bin and the $y$-value is the number of added values that are inside this bin.
\\
\textit{\Large File filter.h}\\
contains classes implementing different conditions for fitted parameters 
(different classes inheriting \textbf{IParamCheck}). \\
All these classes have \textit{Add} for adding information about how to process each parameter of controlled \textbf{ParamSet}.
\\
Abstract class \textbf{FilterRange} allows to store one range per parameter for controlling the values. 
It's inherited by two classes: \\
\textbf{ParamRangeIn} returns \textit{true} if all parameters are inside respective ranges; \\
\textbf{ParamRangeOut} returns \textit{true} if all parameters are outside respective ranges.
\\
Abstract class \textbf{FilterMulti} allows to process several Filters as one. It's inherited by two classes: \textbf{FilterAnd} and \textbf{FilterOr}, that provide respective boolean operations to set of filters.
\\
\textit{\Large File initialconditions.h}\\
contains classes implementing different algorithms providing the initial conditions of fitting algorithm
(different classes inheriting \textbf{IInitialConditions}). \\
All these classes have \textit{Add} for adding information about how to process each parameter or generated \textbf{ParamSet}.
\\
Class \textbf{GenerateUniform} provides generating of \textbf{ParamSet}s with uniformly distributed parameters
(the range for each parameter is given via \textit{Add} method).
\\
Class \textbf{GenerateByGauss} provides generating of \textbf{ParamSet}s with parameters distributed by Gauss 
(the mean value and sigma for each parameter are given via \textit{Add} method).
\\
\textit{\Large File paramfunc.h}\\
provides template classes for building more complicated functions from simpler ones.
\\
Template classes \textbf{PARAMFUNC} and \textbf{PARAM$\_$FUNC} 
require function(s) as constructor parameters.
First one only a function for calculating, the second one also for filter.
\\
Template classes \textbf{ParamFunc} and \textbf{Param$\_$Func} 
require function(s) as template arguments.
Their constructors don't require any parameters.
\\
Described below template classes can be used for providing complicated classes derived from \textbf{IParamFunc}.
As far as all \textbf{IParamFunc} classes provide \textit{()} operator that requires two \textbf{ParamSet}s we will use the following symbols: $X$ means the measured parameters, that are given in \textbf{FitPointsAbstract} instances, $P$ means the fitted parameters that are calculated by \textbf{$\_$gen} instances.
Parameters $F_1$, $F_2$, ... mean that the template class requires another class derived from \textbf{IParamFunc} as a parameter.
\\
\textbf{$Arg<i>$} = $X_i$\\
\textbf{$Par<i>$} = $P_i$\\
\textbf{$PowPar<i,j>$} = $X_i^{P_j}$\\
\textbf{$PowArg<i,j>$} = $X_i^{X_j}$\\
\textbf{$PolynomFunc<i,j,n>$} = $\sum_{k=0}^{n}P_{j+k}X_i^k$\\
\textbf{$Func<f,F_1>$} = $f(F_1(X,P))$; f - double f(double)\\
\textbf{$Func2<f,F_1,F_2>$} = $f(F_1(X,P),F_2(X,P))$; f - double f(double,double)\\
\textbf{$Func3<f,F_1,F_2,F_3>$} = $f(F_1(X,P),F_2(X,P),F_3(X,P))$; f - double f(double,double,double)\\
...and up to function of 6 parameters...\\
\textbf{$ArgShift<F_1,i,j>$} = $F_1(X^*,P),~where~X^*_i=X_i+P_j~and~X^*_k=X_k, k \neq i$\\
\textbf{$ArgScale<F_1,i,j>$} = $F_1(X^*,P),~where~X^*_i=X_i*P_j~and~X^*_k=X_k, k \neq i$\\
\textbf{$Add<F_1,F_2>$} = $F_1(X,P)+F_2(X,P)$\\
\textbf{$Sub<F_1,F_2>$} = $F_1(X,P)-F_2(X,P)$\\
\textbf{$Mul<F_1,F_2>$} = $F_1(X,P)*F_2(X,P)$\\
\textbf{$Div<F_1,F_2>$} = $F_1(X,P)/F_2(X,P)$\\
\textbf{$Power<F_1,F_2>$} = $F_1(X,P)^{F_2(X,P)}$\\
For all these template classes that are derived from their template argument(s) method that provides parameter filter is implemented such way that selects parameters that are good for all inherited parameters.
\end{document}